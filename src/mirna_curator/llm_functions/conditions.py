"""
Here, we define functions using the LLM to check the conditions in the 
flowchart. Each function name must match the name given for the 
condition in the flowchart
"""

import guidance
from guidance import gen, select, system, user, assistant, with_temperature, substring

from mirna_curator.llm_functions.evidence import extract_evidence
from mirna_curator.apis import epmc
from mirna_curator.model.llm import STOP_TOKENS
from mirna_curator.llm_functions.tools import AVAILABLE_TOOLS
import typing as ty


def _select_targets_loop(llm, available_genes):
    """Helper to run the target selection loop"""
    llm += "Protein name(s): "
    while True:
        llm += select(available_genes, name='protein_name', list_append=True)
        last_target = llm['protein_name'][-1]
        if last_target in available_genes:
            available_genes.remove(last_target)
        
        llm += select([" and ", "."], name="multi_target_conjunction")
        if llm["multi_target_conjunction"] == ".":
            break
    return llm

@guidance
def prompted_flowchart_step_bool(
    llm: guidance.models.Model,
    article_text: str,
    load_article_text: bool,
    step_prompt: str,
    rna_id: str,
    config: ty.Optional[ty.Dict[str, ty.Any]] = None,
    temperature_reasoning: ty.Optional[float] = 0.6,
    temperature_selection: ty.Optional[float] = 0.4,
) -> guidance.models.Model:
    """
    Use the given prompt on the article text to answer a yes/no question,
    returning a boolean
    """
    if config is None:
        config = {}

    with user():
        llm += f"You will be asked a yes/no question. The answer could be in following text, or it could be in some text you have already seen.\n"
        if load_article_text:
            llm += f"Text to consider: \n{article_text}\n\n"
        else:
            llm += "Text to consider is included above\n\n"
        llm += f"Question: {step_prompt}\nRestrict your considerations to {rna_id} if there are multiple RNAs mentioned\n"

        llm += "Explain your reasoning step-by-step. Be concise\n"
    with assistant():
        llm += "Reasoning:\n"
        if config.get("deepseek_mode"):
            llm += "<think>\n"
        llm += (
            with_temperature(
                gen(
                    "reasoning",
                    max_tokens=1024,
                    stop=STOP_TOKENS,
                ),
                temperature_reasoning,
            )
            + "\n"
        )

    with assistant():
        llm += f"The final answer, based on my reasoning above is: " + with_temperature(
            select(["yes", "no"], name="answer"), temperature_selection
        )

    llm += extract_evidence(
        article_text, mode=config.get("evidence_mode", "single-sentence")
    )

    return llm


@guidance
def prompted_flowchart_step_tool(
    llm: guidance.models.Model,
    article_text: str,
    load_article_text: bool,
    step_prompt: str,
    rna_id: str,
    config: ty.Optional[ty.Dict[str, ty.Any]] = None,
    tools: ty.Optional[ty.List[str]] = None,
    temperature_reasoning: ty.Optional[float] = 0.6,
    temperature_selection: ty.Optional[float] = 0.4,
) -> guidance.models.Model:
    """
    Use the given prompt on the article text to answer a yes/no question,
    returning a boolean

    Args:
        llm: guidance.models.Model: A guidance model that's ready to go
        article_text: str: The article text we need to work on
        load_article_text: bool : Flag for whether we are going to load the text into the context
        step_prompt: str: The prompt read from the flowchart json file
        rna_id: str: The RNA id we are working on
        tools: ty.Optional[ty.List[str]] = []: A list of tools for the LLM to use
        temperature_reasoning: ty.Optional[float] = 0.6: The reasoning temperature (0.6 is R1 recommended)
        temperature_selection: ty.Optional[float] = 0.4: The yes/no selection temperature
    """
    if config is None:
        config = {}
    if tools is None:
        tools = []

    ## build the tool description string.
    tool_dict = {
        name: AVAILABLE_TOOLS[name] for name in tools if name in AVAILABLE_TOOLS
    }
    tools_string = (
        "To help me answer this question, I have access to some tools to look"
        " up some information. The tools are described here:\n"
        "===========================\n"
    )

    for tool_name, tool in tool_dict.items():
        tools_string += f"Name: {tool_name}\n+++++++++++\n"
        tools_string += f"Description:\n{tool.__doc__}"
        tools_string += "\n+++++++++++\n"

    tools_string += f"Name: finish\n+++++++++++\n"
    tools_string += (
        f"Description:\nEnd the searching process and move on to answering the question"
    )
    tools_string += "\n+++++++++++\n"

    tools_string += "===========================\n"

    _tools = list(tool_dict.keys())
    _tools.append("finish")
    with user():
        if load_article_text:
            llm += f"You will be asked a yes/no question. The answer could be in following text, or it could be in some text you have already seen: \n{article_text}\n\n"
        else:
            llm += "\n\n"

        llm += f"Question: {step_prompt}\n"

    ## Make a tiny little ReAct agent loop
    i = 0
    max_steps = 5
    with assistant():
        llm += tools_string
        while True:
            llm += f"Thought {i}: " + gen(suffix="\n")
            llm += f"Act {i}: " + select(_tools, name="act")
            llm += "[" + gen(name="arg", suffix="]") + "\n"
            if llm["act"].lower() == "finish" or i > max_steps:
                break
            else:
                tool_output = tool_dict[llm["act"]](llm["arg"])
                llm += f"Observation {i}: {tool_output}\n"
            i += 1
        # Restrict your considerations to {rna_id} if there are multiple RNAs mentioned\n"

        llm += "Explain your reasoning step-by-step. Be concise\n"
    with assistant():
        llm += "Reasoning:\n"
        if config.get("deepseek_mode"):
            llm += "<think>\n"
        llm += (
            with_temperature(
                gen(
                    "reasoning",
                    max_tokens=1024,
                    stop=STOP_TOKENS,
                ),
                temperature_reasoning,
            )
            + "\n"
        )

    with assistant():
        llm += f"The final answer, based on my reasoning above is: " + with_temperature(
            select(["yes", "no"], name="answer"), temperature_selection
        )

    llm += extract_evidence(
        article_text, mode=config.get("evidence_mode", "single-sentence")
    )

    return llm


@guidance
def prompted_flowchart_terminal(
    llm: guidance.models.Model,
    article_text: str,
    load_article_text: bool,
    detector_prompt: str,
    rna_id: str,
    paper_id: str,
    config: ty.Optional[ty.Dict[str, ty.Any]] = None,
    temperature_reasoning: ty.Optional[float] = 0.6,
):
    """
    Use the LLM to find the targets and AEs for the GO annotation
    """
    if config is None:
        config = {}

    epmc_annotated_genes = epmc.get_gene_name_annotations(paper_id).copy()
    with user():
        llm += (
            f"You will be asked a question which you must answer using text you have been given. "
            "The answer could be in the text you have already seen, or in the new text below. "
            "If no new text is given, refer to the text you have already seen.\n"
        )
        if load_article_text:
            llm += f"New text: \n{article_text}\n\n"
        else:
            llm += "\n\n"
        llm += (
            f"Question: {detector_prompt}. Restrict your answer to the target(s) of {rna_id}.\n"
            f"Select targets from the following list: {','.join(epmc_annotated_genes)}\n"
            "Ignore targets which do not appear in this list."
        )
    with assistant():
        llm += "Reasoning:\n"
        if config.get("deepseek_mode"):
            llm += "<think>\n"
        llm += (
            with_temperature(
                gen(
                    "detector_reasoning",
                    max_tokens=1024,
                    stop=STOP_TOKENS,
                ),
                temperature_reasoning,
            )
            + "\n"
        )
    with assistant():
        llm = _select_targets_loop(llm, epmc_annotated_genes)

    llm += extract_evidence(
        article_text, mode=config.get("evidence_mode", "single-sentence")
    )

    return llm




@guidance
def prompted_flowchart_terminal_conditional(
    llm: guidance.models.Model,
    article_text: str,
    load_article_text: bool,
    prompt: str,
    rna_id: str,
    paper_id: str,
    config: ty.Optional[ty.Dict[str, ty.Any]] = None,
    temperature_reasoning: ty.Optional[float] = 0.6,
    temperature_selection: ty.Optional[float] = 0.1,
    detector=False,
):
    """
    Use the LLM to find the targets and AEs for the GO annotation, or answer a conditional question
    """
    if config is None:
        config = {}

    if detector:
        epmc_annotated_genes = epmc.get_gene_name_annotations(paper_id).copy()

    with user():
        llm += (
            f"You will be asked a series of questions which you must answer using text you have been given. "
            "The answer could be in the text you have already seen, or in the new text below. "
            "If no new text is given, refer to the text you have already seen.\n"
        )
        if load_article_text:
            llm += f"New text: \n{article_text}\n\n"
        else:
            llm += "\n\n"

        if detector:
            llm += (
            f"Question: {prompt}. Restrict your answer to the target(s) of {rna_id}.\n"
            f"Select targets from the following list: {','.join(epmc_annotated_genes)}\n"
            "Ignore targets which do not appear in this list."
            )
        else:
            llm += (
                f"Question: {prompt}. Restrict your answer to the target(s) of {rna_id}.\n"
            )
    
    with assistant():
        if detector:
            llm += "Reasoning:\n"
            if config.get("deepseek_mode"):
                llm += "<think>\n"
            llm += (
                with_temperature(
                    gen(
                        "detector_reasoning",
                        max_tokens=1024,
                        stop=STOP_TOKENS,
                    ),
                    temperature_reasoning,
                )
                + "\n"
            )
            llm = _select_targets_loop(llm, epmc_annotated_genes)
        else:
            llm += "Reasoning:\n"
            if config.get("deepseek_mode"):
                llm += "<think>\n"
            llm += (
                with_temperature(
                    gen(
                        "reasoning",
                        max_tokens=1024,
                        stop=STOP_TOKENS,
                    ),
                    temperature_reasoning,
                )
                + "\n"
            )
            llm += f"The final answer, based on my reasoning above is: " + with_temperature(
            select(["yes", "no"], name="answer"), temperature_selection)

    llm += extract_evidence(
        article_text, mode=config.get("evidence_mode", "single-sentence")
    )

    return llm
